{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the API"
      ],
      "metadata": {
        "id": "Lrz63tsXTYJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install replicate"
      ],
      "metadata": {
        "id": "k7BQwX1S0XWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import replicate"
      ],
      "metadata": {
        "id": "qWymT7ytZWhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the replicate API token\n",
        "\n",
        "import os\n",
        "#get this token from here https://replicate.com/account/api-tokens\n",
        "os.environ[\"REPLICATE_API_TOKEN\"] = \"r8_3K14qvkrlE2GaUPWEKS5h8qz21vUro03cODrC\""
      ],
      "metadata": {
        "id": "4jw4PumD2vzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading the data"
      ],
      "metadata": {
        "id": "QKWAAXRyVFzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connecting to drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QQ4o6XbE2Ep2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a path in my drive, I dont know if you have to create one of your own and change this"
      ],
      "metadata": {
        "id": "9uJmfp6YWVFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd 'drive/MyDrive/Unipd/CBSD/Data/'"
      ],
      "metadata": {
        "id": "EXVJZcMe4SGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "DQuA0LNtqyQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the Excel files into a pandas DataFrame\n",
        "opinions_en = pd.read_excel(\"DecOp_data_EN_500.xlsx\")\n",
        "opinions_it = pd.read_excel(\"DecOp_data_IT_500.xlsx\")"
      ],
      "metadata": {
        "id": "9poM_yBT5fF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "qBbSNJaUVg2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the datasets' structure\n",
        "opinions_en.head()"
      ],
      "metadata": {
        "id": "v53bzN8-6D1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opinions_en.info()"
      ],
      "metadata": {
        "id": "XZvwReNb7TH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting into training and test sets"
      ],
      "metadata": {
        "id": "GJVA83WuvmVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I could not find the train/test split in the data, but we can do it following\n",
        "# the same criteria that was used in the papers\n",
        "random_seed = 111\n",
        "\n",
        "# Splitting 100 authors for the test set of the english dataset\n",
        "opinions_en_test = opinions_en.sample(n=100, random_state=random_seed)\n",
        "opinions_en_train = opinions_en.drop(opinions_en_test.index)\n",
        "\n",
        "\n",
        "# Splitting 100 authors for the test set of the italian dataset\n",
        "opinions_it_test = opinions_it.sample(n=100, random_state=random_seed)\n",
        "opinions_it_train = opinions_it.drop(opinions_it_test.index)"
      ],
      "metadata": {
        "id": "d3a1otl2Lrqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the paper, they perform a \"cross-topic\" approach, where \"Each Turker was asked to classify 10 opinions randomly extracted from the DecOp test-set as truthful or deceptive in their respective language in a binary response modality. The overall classification performance was computed separately for the US participants and the Italian ones.\""
      ],
      "metadata": {
        "id": "O_hoAw65AwVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reorganizing the datasets to have a \"cross-topic\" dataframe, in which each row\n",
        "# is an opinion. That way, we can run the code sequentially through the examples\n",
        "\n",
        "def long_dataset(df):\n",
        "  topic_list = [('A','GT.A'), ('CL','GT.CL'), ('E','GT.E'),\n",
        "                ('GM','GT.GM'), ('Pom','GT.Pom')]\n",
        "\n",
        "  df_list = []\n",
        "\n",
        "  for topic_label, GT_label in topic_list:\n",
        "    df_topic = df[['ID', 'gender', 'age', topic_label, GT_label]].copy()\n",
        "    df_topic['topic'] = topic_label\n",
        "\n",
        "    df_topic.rename(columns={topic_label: 'opinion', GT_label: 'GT'}, inplace=True)\n",
        "    df_list.append(df_topic)\n",
        "\n",
        "  long_df = pd.concat(df_list, ignore_index=True)\n",
        "  long_df.sort_values(by=['ID','topic'], inplace=True, ignore_index=True)\n",
        "  return long_df\n",
        "\n",
        "opinions_en_test_long = long_dataset(opinions_en_test)\n",
        "opinions_en_train_long = long_dataset(opinions_en_train)\n",
        "\n",
        "opinions_it_test_long = long_dataset(opinions_it_test)\n",
        "opinions_it_train_long = long_dataset(opinions_it_train)"
      ],
      "metadata": {
        "id": "3b9IXHVM9jFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the new structure\n",
        "opinions_en_test_long.head(15)"
      ],
      "metadata": {
        "id": "eSbqwzwEM9Z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the ground truth balance of the splits\n",
        "print(f\"English train ground truth balance: \\n{opinions_en_train_long['GT'].value_counts()}\\n\")\n",
        "print(f\"English test ground truth balance: \\n{opinions_en_test_long['GT'].value_counts()}\\n\")\n",
        "print(f\"Italian train ground truth balance: \\n{opinions_it_train_long['GT'].value_counts()}\\n\")\n",
        "print(f\"Italian test ground truth balance: \\n{opinions_it_test_long['GT'].value_counts()}\\n\")"
      ],
      "metadata": {
        "id": "BOWieY9xNLpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the topics balance of the splits\n",
        "print(f\"English train topic balance: \\n{opinions_en_train_long['topic'].value_counts()}\\n\")\n",
        "print(f\"English test topic balance: \\n{opinions_en_test_long['topic'].value_counts()}\\n\")\n",
        "print(f\"Italian train topic balance: \\n{opinions_it_train_long['topic'].value_counts()}\\n\")\n",
        "print(f\"Italian test topic balance: \\n{opinions_it_test_long['topic'].value_counts()}\\n\")"
      ],
      "metadata": {
        "id": "PeYgkZxr_wRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the topic and truth balance of the splits\n",
        "print(f\"English train topic balance: \\n{opinions_en_train_long.groupby('topic')['GT'].value_counts()}\\n\")\n",
        "print(f\"English test topic balance: \\n{opinions_en_test_long.groupby('topic')['GT'].value_counts()}\\n\")\n",
        "print(f\"Italian train topic balance: \\n{opinions_it_train_long.groupby('topic')['GT'].value_counts()}\\n\")\n",
        "print(f\"Italian test topic balance: \\n{opinions_it_test_long.groupby('topic')['GT'].value_counts()}\\n\")"
      ],
      "metadata": {
        "id": "_Y8cbn7tIqdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtaining random and balanced samples to pass to the model"
      ],
      "metadata": {
        "id": "EA5AqoCdIacg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the paper, they asked 100 people to classify 10 opinions randomly as truthful or deceptive in their respective languages using a binary response modality. Then, they obtained the overall classification performance separately for U.S. and Italian participants. That is equivalent to 1000 samples (per language). However, they are not explicit in saying if the samples could be done with replacement or not. Assuming that they could, drawing the 1000 samples at once would not reproduce the same methodology as the one in the paper, so we are going to follow the paper as closely as possible in this sense.\n",
        "\n",
        "What we can do is get 100 subsets with 10 (randomly sampled) elements each and run them with the model. We do this for each language and obtain the overall accuracy."
      ],
      "metadata": {
        "id": "gSdm76UpIaU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to sample within each group (so that it is balanced)\n",
        "def balanced_sample(group, n, seed):\n",
        "    return group.sample(n, random_state=seed)"
      ],
      "metadata": {
        "id": "wFP55tF8_mvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random seeds to obtain reproducible results (one for each sampling)\n",
        "random_seeds = np.arange(0, 10)\n",
        "\n",
        "# Getting the list of the 100 samples with 10 elements\n",
        "# Since we have 5 topics and 2 labels, we can obtain the subsets\n",
        "# by sampling 1 opinion of each topic and label\n",
        "def get_test_subsets(df):\n",
        "  random_subsets = []\n",
        "  for seed in random_seeds:\n",
        "    balanced_sampled_df = df.groupby(by=['topic','GT'], group_keys=False).apply(balanced_sample, n=1, seed=seed).copy()\n",
        "    balanced_sampled_df = balanced_sampled_df.sample(frac=1)\n",
        "    random_subsets.append(balanced_sampled_df)\n",
        "  return random_subsets\n",
        "\n",
        "opinions_en_test_subsets = get_test_subsets(opinions_en_test_long)\n",
        "opinions_it_test_subsets = get_test_subsets(opinions_it_test_long)"
      ],
      "metadata": {
        "id": "Dpnq2Nw-AEVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the topic and truth balance of the subsets\n",
        "print(f\"English train topic balance: \\n{opinions_en_test_subsets[0].groupby('topic')['GT'].value_counts()}\\n\")\n",
        "print(f\"Italian test topic balance: \\n{opinions_it_test_subsets[5].groupby('topic')['GT'].value_counts()}\\n\")"
      ],
      "metadata": {
        "id": "rUuJPq0YAILB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking a subset\n",
        "opinions_en_test_subsets[4]"
      ],
      "metadata": {
        "id": "yeqjIEmnLIZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative without balancing\n",
        "opinions_en_test_long.sample(10, random_state=10)"
      ],
      "metadata": {
        "id": "2z7ibZpMLwbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero-shot"
      ],
      "metadata": {
        "id": "znX988K7IaGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we will run the prompt without any previous information"
      ],
      "metadata": {
        "id": "1yD2UHXt4nva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the prompting with one example\n",
        "opinion_example = opinions_en_test_long['opinion'][0]\n",
        "output = replicate.run(\"replicate/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
        "    input={\"prompt\": f\"This is the opinion in question: {opinion_example}\",\n",
        "           \"system_prompt\": \"\"\"\n",
        "           I am going to give you a person's opinion about a certain topic.\n",
        "           Tell me if you think that it corresponds to their honest opinion on that topic or not.\n",
        "           Answer only with 'T' (it corresponds to their honest opinion) or 'F' (it does not correspond to their honest opinion).\"\"\",\n",
        "           \"max_new_tokens\": 5})\n",
        "\n",
        "\n",
        "full_response = \"\"\n",
        "\n",
        "for item in output:\n",
        "  full_response += item\n",
        "\n",
        "print(f\"The opinion given to Llama 2 was: {opinion_example}\")\n",
        "print(f\"Llama2s'answer: {full_response}\")"
      ],
      "metadata": {
        "id": "HbiAmcEKv27V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the function to get Llama's answer with respect to each opinion\n",
        "def get_zero_shot_answer(row):\n",
        "  opinion = row['opinion']\n",
        "  output = replicate.run(\"replicate/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
        "                          input={\"prompt\": f\"This is the opinion in question: {opinion}\",\n",
        "                                \"system_prompt\":\"\"\"\n",
        "                                I am going to give you a person's opinion about a certain topic.\n",
        "                                Tell me if you think that it corresponds to their honest opinion on that topic or not.\n",
        "                                Answer only with 'T' (it corresponds to their honest opinion) or 'F' (it does not correspond to their honest opinion).\"\"\",\n",
        "                                \"max_new_tokens\": 5})\n",
        "  full_response = \"\"\n",
        "  for item in output:\n",
        "    full_response += item\n",
        "  return full_response"
      ],
      "metadata": {
        "id": "OJYIAlA_CPeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to run the zero-shot prompt for all the randomly selected subsets\n",
        "def run_zero_shot(subsets_list):\n",
        "  for i, subset in enumerate(subsets_list):\n",
        "    subset['id_run'] = i\n",
        "    subset['LLama2_zero_shot'] = subset.apply(get_zero_shot_answer, axis=1)\n",
        "    print(f\"It has processed {i+1} subsets.\")\n",
        "  df_zero_shot_complete = pd.concat(subsets_list, ignore_index=True)\n",
        "  return df_zero_shot_complete"
      ],
      "metadata": {
        "id": "WdxgG9tNxXm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero-shot for the english dataset"
      ],
      "metadata": {
        "id": "UjIUALj0SgMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Running and checking the results of the english dataset\n",
        "opinions_en_results_zs = run_zero_shot(opinions_en_test_subsets)\n",
        "opinions_en_results_zs.head(15)"
      ],
      "metadata": {
        "id": "TlCi-KnF4GEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can check what were the Llama's responses\n",
        "opinions_en_results_zs['LLama2_zero_shot'].unique()"
      ],
      "metadata": {
        "id": "qqhv-UaxR5Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# By checking, it seems that there are spaces at the beginning or the end\n",
        "# of the Llama column, so maybe it's better to strip the result to compare it\n",
        "# with the ground truth.\n",
        "opinions_en_results_zs['LLama2_zero_shot'] = opinions_en_results_zs['LLama2_zero_shot'].str.strip()"
      ],
      "metadata": {
        "id": "wC9EswylODw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing the zero shot results for the english dataset\n",
        "#opinions_en_results_zs.to_csv('../Results/opinions_en_results_zero_shot_new_run2.csv', index=False)"
      ],
      "metadata": {
        "id": "E0CTgvrsWvc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero-shot for the italian dataset"
      ],
      "metadata": {
        "id": "47JuqmQMSXTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Running and checking the results of the italian dataset\n",
        "opinions_it_results = run_zero_shot(opinions_it_test_subsets)\n",
        "opinions_it_results.head(15)"
      ],
      "metadata": {
        "id": "6S6RCeko9D5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opinions_it_results['LLama2_zero_shot'].unique()"
      ],
      "metadata": {
        "id": "nxA1zyN5Q6ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In this case, we also strip the results\n",
        "opinions_it_results['LLama2_zero_shot'] = opinions_it_results['LLama2_zero_shot'].str.strip()"
      ],
      "metadata": {
        "id": "UuDLuta7Rt20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opinions_it_results['LLama2_zero_shot'].value_counts()"
      ],
      "metadata": {
        "id": "U6_5GLzMSzsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing the zero shot results for the english dataset\n",
        "#opinions_it_results.to_csv('../Results/opinions_it_results_zero_shot_new_run2.csv', index=False)"
      ],
      "metadata": {
        "id": "czpkp0U-F_gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Few-shot"
      ],
      "metadata": {
        "id": "qNaSkHd8J4lS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will do three one-shot approaches:\n",
        "\n",
        "\n",
        "*   Giving two examples in the same language as the test opinion.\n",
        "*   Giving one example in english and one example in italian.\n",
        "*   Giving two examples in a different language (with respect to the test opinion's language).\n",
        "\n"
      ],
      "metadata": {
        "id": "Hpw7LMErVIBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the prompting with one example\n",
        "opinion_test = opinions_en_test_long['opinion'][96]\n",
        "\n",
        "row_opinion_example1 = opinions_en_train_long.iloc[0]\n",
        "row_opinion_example2 = opinions_en_train_long.iloc[12]\n",
        "output_fs_en = replicate.run(\"replicate/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
        "    input={\"prompt\": f\"This is the test opinion: {opinion_test}\",\n",
        "           \"system_prompt\": f\"\"\"\n",
        "           I am going to give you a person's opinion about a certain topic. This will be the test opinion.\n",
        "           Tell me if you think that it corresponds to their honest opinion on that topic or not.\n",
        "           Answer only with 'T' (it corresponds to their honest opinion) or 'F' (it does not correspond to their honest opinion).\n",
        "\n",
        "           As an example, I will give you two different sample opinions and their corresponding label ('T' or 'F'):\n",
        "           - Sample opinion 1: {row_opinion_example1['opinion']}\n",
        "           - Label of opinion 1: {row_opinion_example1['GT']}\n",
        "\n",
        "           - Sample opinion 2: {row_opinion_example2['opinion']}\n",
        "           - Label of opinion 2: {row_opinion_example2['GT']}\n",
        "\n",
        "           Please, only respond regarding the opinion of interest. The output should only be one letter ('T' or 'F')\n",
        "           \"\"\",\n",
        "           \"max_new_tokens\": 5})\n",
        "\n",
        "\n",
        "full_response_fs = \"\"\n",
        "\n",
        "for item in output_fs_en:\n",
        "  full_response_fs += item\n",
        "\n",
        "print(f\"The opinion given to Llama 2 was: {opinion_test}\")\n",
        "print(f\"Llama2s'answer: {full_response_fs}\")"
      ],
      "metadata": {
        "id": "VgB6ckwLVC-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the function to get Llama's answer with respect to each opinion\n",
        "def get_few_shot_answer(row_opinion_test, language_examples):\n",
        "  # Assigning the examples to show the LLM before doign the test,\n",
        "  # depending on the type of task\n",
        "  if language_examples == 'english':\n",
        "    opinion_example1 = row_opinion_test['opinion_example_en1']\n",
        "    opinion_example2 = row_opinion_test['opinion_example_en2']\n",
        "    GT_example1 = row_opinion_test['GT_example_en1']\n",
        "    GT_example2 = row_opinion_test['GT_example_en2']\n",
        "  elif language_examples == 'italian':\n",
        "    opinion_example1 = row_opinion_test['opinion_example_it1']\n",
        "    opinion_example2 = row_opinion_test['opinion_example_it2']\n",
        "    GT_example1 = row_opinion_test['GT_example_it1']\n",
        "    GT_example2 = row_opinion_test['GT_example_it2']\n",
        "  elif language_examples == 'mixed':\n",
        "    opinion_example1 = row_opinion_test['opinion_example_en1']\n",
        "    opinion_example2 = row_opinion_test['opinion_example_it1']\n",
        "    GT_example1 = row_opinion_test['GT_example_en1']\n",
        "    GT_example2 = row_opinion_test['GT_example_it1']\n",
        "\n",
        "  # Getting the test opinion\n",
        "  opinion_test = row_opinion_test['opinion']\n",
        "\n",
        "\n",
        "  output = replicate.run(\"replicate/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
        "                          input={\"prompt\": f\"This is the test opinion: {opinion_test}\",\n",
        "                                 \"system_prompt\": f\"\"\"\n",
        "           I am going to give you a person's opinion about a certain topic. This will be the test opinion.\n",
        "           Tell me if you think that it corresponds to their honest opinion on that topic or not.\n",
        "           Answer only with 'T' (it corresponds to their honest opinion) or 'F' (it does not correspond to their honest opinion).\n",
        "\n",
        "           As an example, I will give you two different sample opinions and their corresponding label ('T' or 'F'):\n",
        "           - Sample opinion 1: {opinion_example1}\n",
        "           - Label of opinion 1: {GT_example1}\n",
        "\n",
        "           - Sample opinion 2: {opinion_example2}\n",
        "           - Label of opinion 2: {GT_example2}\n",
        "\n",
        "           Please, only respond regarding the opinion of interest. The output should only be one letter ('T' or 'F')\n",
        "           \"\"\",\n",
        "           \"max_new_tokens\": 5})\n",
        "\n",
        "  full_response = \"\"\n",
        "  for item in output:\n",
        "    full_response += item\n",
        "\n",
        "  return full_response"
      ],
      "metadata": {
        "id": "5pAmqh1-IY2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the examples for each case"
      ],
      "metadata": {
        "id": "EX1MTK9zLH31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random seeds to obtain reproducible results (one for each sampling)\n",
        "# Adding the examples as a columns to the test dataframes.\n",
        "# Each dataframe will receive four examples: two in english and two in italian\n",
        "# Depending on the task, when prompting Llama, we select two of the examples.\n",
        "def get_examples_few_shot(subsets_list):\n",
        "\n",
        "  # Getting one seed for each subset (and they are different for each example)\n",
        "  random_seed_examples_en1 = np.arange(0, 10)\n",
        "  random_seed_examples_en2 = np.arange(10, 20)\n",
        "  random_seed_examples_it1 = np.arange(20, 30)\n",
        "  random_seed_examples_it2 = np.arange(30, 40)\n",
        "\n",
        "\n",
        "  subsets_list_result = []\n",
        "  for i, subset in enumerate(subsets_list):\n",
        "    opinion_GT_example_en1 = opinions_en_train_long[['opinion','GT']].sample(10, random_state=random_seed_examples_en1[i]).reset_index(drop=True)\n",
        "    opinion_GT_example_en1.rename(columns={'opinion':'opinion_example_en1','GT':'GT_example_en1'}, inplace=True)\n",
        "\n",
        "    opinion_GT_example_en2 = opinions_en_train_long[['opinion','GT']].sample(10, random_state=random_seed_examples_en2[i]).reset_index(drop=True)\n",
        "    opinion_GT_example_en2.rename(columns={'opinion':'opinion_example_en2','GT':'GT_example_en2'}, inplace=True)\n",
        "\n",
        "    opinion_GT_example_it1 = opinions_it_train_long[['opinion','GT']].sample(10, random_state=random_seed_examples_it1[i]).reset_index(drop=True)\n",
        "    opinion_GT_example_it1.rename(columns={'opinion':'opinion_example_it1','GT':'GT_example_it1'}, inplace=True)\n",
        "\n",
        "    opinion_GT_example_it2 = opinions_it_train_long[['opinion','GT']].sample(10, random_state=random_seed_examples_it2[i]).reset_index(drop=True)\n",
        "    opinion_GT_example_it2.rename(columns={'opinion':'opinion_example_it2','GT':'GT_example_it2'}, inplace=True)\n",
        "\n",
        "    subset = pd.concat([subset.reset_index(drop=True),\n",
        "                        opinion_GT_example_en1, opinion_GT_example_en2,\n",
        "                        opinion_GT_example_it1, opinion_GT_example_it2],\n",
        "                       axis=1)\n",
        "    subsets_list_result.append(subset)\n",
        "  return subsets_list_result"
      ],
      "metadata": {
        "id": "GuPO6FwQJ5PG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Executing the function to extract the examples for each observation\n",
        "opinions_en_test_subsets = get_examples_few_shot(opinions_en_test_subsets)\n",
        "opinions_it_test_subsets = get_examples_few_shot(opinions_it_test_subsets)"
      ],
      "metadata": {
        "id": "8uUrProxqlMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the new structure\n",
        "opinions_en_test_subsets[4]"
      ],
      "metadata": {
        "id": "uKACEFDXSaj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to run the few-shot prompt for all the randomly selected subsets\n",
        "# It can be done with two examples in english, two examples in italian and two\n",
        "# mixed-language examples\n",
        "def run_few_shot(subsets_list, language_examples):\n",
        "  for i, subset in enumerate(subsets_list):\n",
        "    col_name = 'LLama2_few_shot_' + language_examples\n",
        "    subset['id_run'] = i\n",
        "    subset[col_name] = subset.apply(get_few_shot_answer, language_examples=language_examples, axis=1)\n",
        "    print(f\"It has processed {i+1} subsets.\")\n",
        "  df_few_shot_complete = pd.concat(subsets_list, ignore_index=True)\n",
        "  return df_few_shot_complete"
      ],
      "metadata": {
        "id": "pIaX7PkGhF86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Few-shot for the english dataset"
      ],
      "metadata": {
        "id": "AjxHnzWWsp3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Two examples in english"
      ],
      "metadata": {
        "id": "hXqlSZ0MLU2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Running and checking the results of the english dataset with the within language few-shot\n",
        "opinions_en_results_fs_within = run_few_shot(opinions_en_test_subsets, language_examples='english')\n",
        "opinions_en_results_fs_within.head(15)"
      ],
      "metadata": {
        "id": "a8u1m5N8lXJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can check what were the Llama's responses\n",
        "opinions_en_results_fs_within['LLama2_few_shot_english'].unique()"
      ],
      "metadata": {
        "id": "0xt5sE7X_a3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In this case, we also strip the results\n",
        "opinions_en_results_fs_within['LLama2_few_shot_english'] = opinions_en_results_fs_within['LLama2_few_shot_english'].str.strip()"
      ],
      "metadata": {
        "id": "lutvblV-_zb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing the one-shot results for the english dataset and the within language test\n",
        "#opinions_en_results_fs_within.to_csv('../Results/opinions_en_results_fs_within.csv', index=False)"
      ],
      "metadata": {
        "id": "kyS6VZ0nqtqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Two examples in italian"
      ],
      "metadata": {
        "id": "VyN5FgScAPuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Running and checking the results of the english dataset with the few-shot in other language\n",
        "opinions_en_results_fs_other = run_few_shot(opinions_en_test_subsets, language_examples='italian')\n",
        "opinions_en_results_fs_other.head(15)"
      ],
      "metadata": {
        "id": "dHbwdlh-ta80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can check what were the Llama's responses\n",
        "opinions_en_results_fs_other['LLama2_few_shot_italian'].unique()"
      ],
      "metadata": {
        "id": "AaXMD40_tavr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In this case, we also strip the results\n",
        "opinions_en_results_fs_other['LLama2_few_shot_italian'] = opinions_en_results_fs_other['LLama2_few_shot_italian'].str.strip()"
      ],
      "metadata": {
        "id": "65cMQcr8ANzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing the one-shot results for the english dataset and the \"other\" language test\n",
        "#opinions_en_results_fs_other.to_csv('../Results/opinions_en_results_fs_other.csv', index=False)"
      ],
      "metadata": {
        "id": "TqSsByNlADG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Two mixed-language examples"
      ],
      "metadata": {
        "id": "ayvupo8RAPig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Running and checking the results of the english dataset with the mixed language few-shot\n",
        "opinions_en_results_fs_mixed = run_few_shot(opinions_en_test_subsets, language_examples='mixed')\n",
        "opinions_en_results_fs_mixed.head(15)"
      ],
      "metadata": {
        "id": "QMqD39-GtiOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can check what were the Llama's responses\n",
        "opinions_en_results_fs_mixed['LLama2_few_shot_mixed'].unique()"
      ],
      "metadata": {
        "id": "TDVzcEkJBvdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In this case, we also strip the results\n",
        "opinions_en_results_fs_mixed['LLama2_few_shot_mixed'] = opinions_en_results_fs_mixed['LLama2_few_shot_mixed'].str.strip()"
      ],
      "metadata": {
        "id": "lQYgdbJlB2py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing the one shot results for the english dataset and the mixed language test\n",
        "#opinions_en_results_fs_mixed.to_csv('../Results/opinions_en_results_fs_mixed.csv', index=False)"
      ],
      "metadata": {
        "id": "r3Xx_zJrtiLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Few-shot for the italian dataset"
      ],
      "metadata": {
        "id": "5U8BYUtmAPfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Two examples in italian"
      ],
      "metadata": {
        "id": "gWDVgqm7APTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Running and checking the results of the italian dataset with the within language few-shot\n",
        "opinions_it_results_fs_within = run_few_shot(opinions_it_test_subsets, language_examples='italian')\n",
        "opinions_it_results_fs_within.head(15)"
      ],
      "metadata": {
        "id": "Sv3Rk3rotwHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can check what were the Llama's responses\n",
        "opinions_it_results_fs_within['LLama2_few_shot_italian'].unique()"
      ],
      "metadata": {
        "id": "X16R1Gq1CEQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In this case, we also strip the results\n",
        "opinions_it_results_fs_within['LLama2_few_shot_italian'] = opinions_it_results_fs_within['LLama2_few_shot_italian'].str.strip()"
      ],
      "metadata": {
        "id": "Hszp4DqZCECT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing the one shot results for the english dataset and the within language test\n",
        "#opinions_it_results_fs_within.to_csv('../Results/opinions_it_results_fs_within.csv', index=False)"
      ],
      "metadata": {
        "id": "Bbm75PcatwEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Two examples in english"
      ],
      "metadata": {
        "id": "Q9xOGzvaAPP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Running and checking the results of the italian dataset with the \"other\" language few-shot\n",
        "opinions_it_results_fs_other = run_few_shot(opinions_it_test_subsets, language_examples='english')\n",
        "opinions_it_results_fs_other.head(15)"
      ],
      "metadata": {
        "id": "XP-oWiUgt2yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can check what were the Llama's responses\n",
        "opinions_it_results_fs_other['LLama2_few_shot_english'].unique()"
      ],
      "metadata": {
        "id": "w-03Hhs6C2im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In this case, we also strip the results\n",
        "opinions_it_results_fs_other['LLama2_few_shot_english'] = opinions_it_results_fs_other['LLama2_few_shot_english'].str.strip()"
      ],
      "metadata": {
        "id": "avPfjP5oC4Ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing the one-shot results for the italian dataset and the \"other\" language test\n",
        "#opinions_it_results_fs_other.to_csv('../Results/opinions_it_results_fs_other.csv', index=False)"
      ],
      "metadata": {
        "id": "UFJQL4Lht2vD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Two mixed-language examples"
      ],
      "metadata": {
        "id": "BNevHFGEAPDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Running and checking the results of the italian dataset with the mixed language test\n",
        "opinions_it_results_fs_mixed = run_few_shot(opinions_it_test_subsets, language_examples='mixed')\n",
        "opinions_it_results_fs_mixed.head(15)"
      ],
      "metadata": {
        "id": "mIbla9YBynqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can check what were the Llama's responses\n",
        "opinions_it_results_fs_mixed['LLama2_few_shot_mixed'].unique()"
      ],
      "metadata": {
        "id": "OcjnkHFVDNRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In this case, we also strip the results\n",
        "opinions_it_results_fs_mixed['LLama2_few_shot_mixed'] = opinions_it_results_fs_mixed['LLama2_few_shot_mixed'].str.strip()"
      ],
      "metadata": {
        "id": "aftH-VHtDNF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing the one-shot results for the italian dataset and the mixed language test\n",
        "#opinions_it_results_fs_mixed.to_csv('../Results/opinions_it_results_fs_mixed.csv', index=False)"
      ],
      "metadata": {
        "id": "4HspVWqWynYV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}